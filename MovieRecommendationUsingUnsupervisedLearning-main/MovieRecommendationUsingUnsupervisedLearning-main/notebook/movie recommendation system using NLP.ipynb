{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8fe062",
   "metadata": {},
   "source": [
    "# ðŸŽ¬ Advanced Content-Based Movie Recommendation Engine (NLP)\n",
    "**By: Vishwaraj Khatpe | Final Year Engineering Student (AIML)**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Executive Summary\n",
    "This project addresses the \"cold start\" problem in recommendation systemsâ€”how to suggest movies to users without needing their historical viewing data. By leveraging Natural Language Processing (NLP) techniques on movie plot summaries, this engine identifies semantic similarities between films that simple genre-matching cannot capture.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Technical Approach\n",
    "Instead of traditional metadata clustering, this system utilizes a Vector Space Model:\n",
    "\n",
    "* **Text Ingestion:** Processing raw text from movie \"overviews\" (plot summaries).\n",
    "* **TF-IDF Vectorization:** Converting unstructured text into a mathematical feature matrix, weighing unique plot points higher than common words.\n",
    "* **Cosine Similarity:** Calculating the angular distance between movie vectors to quantify semantic relatedness.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why This Approach?\n",
    "\n",
    "* **Granularity:** While *Interstellar* and *Star Wars* are both \"Sci-Fi\", NLP can distinguish that one is about \"black holes/physics\" and the other is a \"space opera.\"\n",
    "* **Scalability:** Matrix-based filtering is highly optimized for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea88c9",
   "metadata": {},
   "source": [
    "### [Code] - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b9800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries for Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Natural Language Processing & Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel # Faster than cosine_similarity for dense matrices\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "print(\" Libraries Imported Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd09c5",
   "metadata": {},
   "source": [
    "## 4. Data Ingestion & Cleaning\n",
    "\n",
    "We load the dataset and perform critical cleaning steps to prepare the text data for vectorization.\n",
    "\n",
    "### Key Decisions:\n",
    "\n",
    "* **Memory Management:** The dataset can be large, so we process only necessary columns.\n",
    "* **Text Sanitization:** Null values in text columns cause vectorizers to crash. We must fill NaN values with empty strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32e0fd",
   "metadata": {},
   "source": [
    "### [Code] - Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e1d637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Shape: (45466, 24)\n",
      "Cleaned Dataset Shape: (45460, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "overview",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "84a44103-501e-4c64-b09b-18bcea8a64d0",
       "rows": [
        [
         "0",
         "Toy Story",
         "Led by Woody, Andy's toys live happily in his room until Andy's birthday brings Buzz Lightyear onto the scene. Afraid of losing his place in Andy's heart, Woody plots against Buzz. But when circumstances separate Buzz and Woody from their owner, the duo eventually learns to put aside their differences."
        ],
        [
         "1",
         "Jumanji",
         "When siblings Judy and Peter discover an enchanted board game that opens the door to a magical world, they unwittingly invite Alan -- an adult who's been trapped inside the game for 26 years -- into their living room. Alan's only hope for freedom is to finish the game, which proves risky as all three find themselves running from giant rhinoceroses, evil monkeys and other terrifying creatures."
        ],
        [
         "2",
         "Grumpier Old Men",
         "A family wedding reignites the ancient feud between next-door neighbors and fishing buddies John and Max. Meanwhile, a sultry Italian divorcÃ©e opens a restaurant at the local bait shop, alarming the locals who worry she'll scare the fish away. But she's less interested in seafood than she is in cooking up a hot time with Max."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                                           overview\n",
       "0         Toy Story  Led by Woody, Andy's toys live happily in his ...\n",
       "1           Jumanji  When siblings Judy and Peter discover an encha...\n",
       "2  Grumpier Old Men  A family wedding reignites the ancient feud be..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"../data/movies_metadata.csv\", low_memory=False)\n",
    "    print(f\"Original Dataset Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'movies_metadata.csv' not found. Please upload the file.\")\n",
    "\n",
    "# Select distinct features for the recommendation engine\n",
    "# 'overview' is our primary feature for NLP\n",
    "selected_cols = ['title', 'overview', 'genres', 'vote_average', 'popularity']\n",
    "movies = df[selected_cols].copy()\n",
    "\n",
    "# Data Cleaning\n",
    "# 1. Drop movies with no title (critical identifier)\n",
    "movies = movies.dropna(subset=['title'])\n",
    "\n",
    "# 2. Fill Null Plot Summaries\n",
    "# TF-IDF cannot handle NaN; we replace them with empty strings\n",
    "movies['overview'] = movies['overview'].fillna('')\n",
    "\n",
    "# 3. Clean numeric columns for display purposes later\n",
    "movies['popularity'] = pd.to_numeric(movies['popularity'], errors='coerce')\n",
    "movies['vote_average'] = pd.to_numeric(movies['vote_average'], errors='coerce')\n",
    "\n",
    "# Reset index to ensure robust lookups later\n",
    "movies = movies.reset_index(drop=True)\n",
    "\n",
    "print(f\"Cleaned Dataset Shape: {movies.shape}\")\n",
    "movies[['title', 'overview']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef2b4b",
   "metadata": {},
   "source": [
    "# **NLP Feature Engineering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477b6b3",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering: TF-IDF Vectorization\n",
    "\n",
    "To compare plot summaries mathematically, we must convert text to numbers. We use **TF-IDF (Term Frequency-Inverse Document Frequency)**.\n",
    "\n",
    "### Technical Insight: Why TF-IDF and not simple Word Counts?\n",
    "\n",
    "* **Term Frequency (TF):** Counts how often a word appears in a specific movie plot.\n",
    "* **Inverse Document Frequency (IDF):** Penalizes words that appear in every movie (like \"the\", \"movie\", \"plot\"). This ensures that generic words don't dominate the similarity score, allowing unique keywords (e.g., \"wizard\", \"dinosaur\", \"heist\") to drive recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b5165",
   "metadata": {},
   "source": [
    "### [Code] - TF-IDF Matrix Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026d812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing TF-IDF Matrix... (This may take a moment)\n",
      "TF-IDF Matrix Created. Shape: (45460, 75827)\n",
      "- Analysis: We are comparing 45460 movies across 75827 unique words.\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "# stop_words='english': Removes common words like 'the', 'a', 'in'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "print(\"Constructing TF-IDF Matrix... (This may take a moment)\")\n",
    "\n",
    "# Fit and Transform the 'overview' column\n",
    "# This creates a matrix where Rows = Movies, Columns = Unique Words\n",
    "tfidf_matrix = tfidf.fit_transform(movies['overview'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Created. Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"- Analysis: We are comparing {tfidf_matrix.shape[0]} movies across {tfidf_matrix.shape[1]} unique words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b3af8",
   "metadata": {},
   "source": [
    "## 6. Computing Similarity\n",
    "\n",
    "With our text converted to vectors, we calculate the similarity between every pair of movies.\n",
    "\n",
    "We use the **Linear Kernel** (equivalent to Cosine Similarity for normalized data). This computes the cosine of the angle between two vectors:\n",
    "\n",
    "* **Score = 1.0**: The plots are identical.\n",
    "* **Score = 0.0**: The plots share no meaningful words.\n",
    "\n",
    "\n",
    "\n",
    "> **Note:** The resulting matrix is massive ($N \\times N$ movies), containing the similarity score of every movie against every other movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a4833",
   "metadata": {},
   "source": [
    "### [Code] - Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44f51eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Computing Cosine Similarity Matrix...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.4 GiB for an array with shape (45460, 45460) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Computing Cosine Similarity Matrix...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Compute the linear kernel (dot product) of the matrix\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Since TF-IDF output is normalized, this is equivalent to Cosine Similarity\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m cosine_sim \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Similarity Matrix Computed. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcosine_sim\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a reverse mapping of indices and movie titles\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# This allows us to look up the index of a movie by its title\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vishwaraj khatpe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vishwaraj khatpe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1434\u001b[0m, in \u001b[0;36mlinear_kernel\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03mCompute the linear kernel between X and Y.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;124;03m       [1., 2.]])\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1433\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m-> 1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vishwaraj khatpe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\extmath.py:211\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    203\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    206\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m ):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\vishwaraj khatpe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1170\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1170\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vishwaraj khatpe\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\sparse\\_base.py:1366\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.4 GiB for an array with shape (45460, 45460) and data type float64"
     ]
    }
   ],
   "source": [
    "print(\" Computing Cosine Similarity Matrix...\")\n",
    "\n",
    "# Compute the linear kernel (dot product) of the matrix\n",
    "# Since TF-IDF output is normalized, this is equivalent to Cosine Similarity\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(f\" Similarity Matrix Computed. Shape: {cosine_sim.shape}\")\n",
    "\n",
    "# Create a reverse mapping of indices and movie titles\n",
    "# This allows us to look up the index of a movie by its title\n",
    "indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1a2c4",
   "metadata": {},
   "source": [
    "# **Inference Engine**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166de653",
   "metadata": {},
   "source": [
    "## 7. Building the Recommendation Function\n",
    "\n",
    "This function encapsulates the core logic of the system through a four-step process:\n",
    "\n",
    "1.  **Lookup**: Find the unique index of the user's input movie title in the metadata DataFrame.\n",
    "2.  **Similarity Retrieval**: Access the specific row of similarity scores for that movie from the `cosine_sim` matrix.\n",
    "3.  **Ranking**: Sort all movies based on their similarity scores in descending order to identify the closest matches.\n",
    "4.  **Fetch**: Filter out the input movie itself and return the **top 10** most similar movie titles.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "> **Implementation Note:** Ensure that the input title is pre-processed or matched using a case-insensitive search to prevent `IndexErrors` during the lookup phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd50e5a",
   "metadata": {},
   "source": [
    "### [Code] - Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfe3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    \"\"\"\n",
    "    Returns top 10 recommended movies based on plot similarity.\n",
    "    \n",
    "    Args:\n",
    "        title (str): Title of the movie (Case sensitive in this basic version).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Similar movies with their plot overviews.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Get the index of the movie that matches the title\n",
    "        idx = indices[title]\n",
    "        \n",
    "        # Handle duplicate titles (if any), taking the first one\n",
    "        if isinstance(idx, pd.Series):\n",
    "            idx = idx.iloc[0]\n",
    "\n",
    "        # 2. Get the pairwsie similarity scores of all movies with that movie\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "        # 3. Sort the movies based on the similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 4. Get the scores of the 10 most similar movies (ignoring the first one which is itself)\n",
    "        sim_scores = sim_scores[1:11]\n",
    "\n",
    "        # 5. Get the movie indices\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "        # 6. Return the top 10 most similar movies\n",
    "        return movies[['title', 'vote_average', 'overview']].iloc[movie_indices]\n",
    "        \n",
    "    except KeyError:\n",
    "        return \"âš ï¸ Movie not found. Please check the spelling exactly as it appears in the database.\"\n",
    "\n",
    "# --- Test Case ---\n",
    "test_movie = 'The Dark Knight Rises'\n",
    "print(f\"ðŸŽ¬ Recommendations for '{test_movie}':\")\n",
    "get_recommendations(test_movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1617dec",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1b9ef0",
   "metadata": {},
   "source": [
    "# 8. Conclusion & Future Improvements\n",
    "\n",
    "## Project Summary\n",
    "We successfully built a **Content-Based Filtering System** using Natural Language Processing (NLP). By processing plot summaries with **TF-IDF**, we enabled the system to \"read\" movie descriptions and understand semantic relationships, providing recommendations that go deeper than simple genre matching.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Enhancements (Interview Talking Points)\n",
    "To scale this project for a production environment, I would consider the following improvements:\n",
    "\n",
    "### 1. Hybridization\n",
    "Combining this **Content-Based** score with a **Collaborative Filtering (SVD)** model to incorporate user personalization and historical behavior.\n",
    "\n",
    "### 2. Advanced Embeddings\n",
    "Replacing TF-IDF with **BERT** or **Word2Vec** embeddings. This allows the model to capture deeper context (e.g., understanding the semantic relationship between terms like \"king\" and \"queen\").\n",
    "\n",
    "### 3. Approximate Nearest Neighbors (ANN)\n",
    "For datasets with millions of movies, calculating the full Cosine Similarity matrix is computationally expensive. Using libraries like **FAISS** or **Annoy** can reduce inference time:\n",
    "\n",
    "* **Current complexity:** $O(N)$\n",
    "* **Optimized complexity:** $O(\\log N)$\n",
    "\n",
    "> **Pro-Tip:** Mentioning ANN in interviews demonstrates that you understand the difference between a \"laptop-scale\" project and a \"production-scale\" system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
