{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Segmentation Using Unsupervised Learning\n",
        "\n",
        "\n",
        "**Abstract:** \n",
        "In the competitive retail landscape, mass marketing is often inefficient. This project applies **K-Means Clustering** to segment mall customers based on demographic and behavioral data (Age, Income, Spending Score). By moving from a generalist approach to data-driven micro-segmentation, we aim to derive actionable insights for personalized marketing strategies.\n",
        "\n",
        "**Methodology:**\n",
        "1. **Exploratory Data Analysis (EDA):** Univariate distributions and Multivariate correlation analysis.\n",
        "2. **Data Preprocessing:** Feature selection and Z-score Normalization (StandardScaler) to handle variance disparities.\n",
        "3. **Model Optimization:** Using the **Elbow Method** (Inertia) and **Silhouette Analysis** to determine the optimal number of clusters ($k$).\n",
        "4. **Modeling:** Implementation of K-Means with `k-means++` initialization to ensure convergence.\n",
        "5. **3D Visualization:** Plotting clusters in a 3-dimensional space for interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Library Import & Environment Setup\n",
        "We utilize `Pandas` for manipulation, `Seaborn` for statistical visualization, and `Plotly` for interactive 3D plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Configuration for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"deep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Loading & Statistical Audit\n",
        "The dataset contains basic information about customers, including their unique ID, Gender, Age, Annual Income, and Spending Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Mall_Customers.csv\")\n",
        "\n",
        "# Checking data structure\n",
        "print(f\"Dataset Dimensions: {df.shape}\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Statistical Summary:** \n",
        "We analyze the central tendency and dispersion of the data. This helps identify if scaling is required (e.g., if Income ranges from 15-137 while Age ranges from 18-70)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(df.describe().T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Exploratory Data Analysis (EDA)\n",
        "#### 3.1 Univariate Analysis: Distributions\n",
        "Before clustering, we must understand the underlying distribution of each feature. We use Kernel Density Estimation (KDE) to visualize the probability density."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "sns.histplot(df['Age'], kde=True, ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Distribution of Age')\n",
        "\n",
        "sns.histplot(df['Annual Income (k$)'], kde=True, ax=axes[1], color='orange')\n",
        "axes[1].set_title('Distribution of Income')\n",
        "\n",
        "sns.histplot(df['Spending Score (1-100)'], kde=True, ax=axes[2], color='green')\n",
        "axes[2].set_title('Distribution of Spending Score')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2 Bivariate Analysis: Correlation Heatmap\n",
        "We check for multicollinearity. In Clustering, highly correlated features might bias the distance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "# Dropping CustomerID and Gender for correlation matrix\n",
        "corr_matrix = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Data Preprocessing\n",
        "**Why Scaling?** \n",
        "K-Means calculates the Euclidean distance between points. If one feature has a large range (e.g., Income: 0-137,000) and another has a small range (e.g., Age: 0-100), the algorithm will be biased towards Income.\n",
        "\n",
        "We use **StandardScaler** to transform data such that $\\mu = 0$ and $\\sigma = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
        "X = df[features]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Data standardization complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Model Selection: Finding Optimal $K$\n",
        "We employ two techniques to determine the number of clusters:\n",
        "1. **The Elbow Method:** Measures the Within-Cluster Sum of Squares (WCSS). We look for the point where the decrease in WCSS slows down (the \"elbow\").\n",
        "2. **Silhouette Score:** (Optional but recommended) Measures how similar an object is to its own cluster compared to other clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculating WCSS for K=1 to K=10\n",
        "wcss = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "    \n",
        "    # Silhouette score is only valid for k > 1\n",
        "    if k > 1:\n",
        "        score = silhouette_score(X_scaled, kmeans.labels_)\n",
        "        silhouette_scores.append(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Visualizing the Elbow Curve:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, 11), wcss, marker='o', linestyle='--', color='crimson')\n",
        "plt.title('Elbow Method: WCSS vs Number of Clusters')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS (Inertia)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:** The plot shows a clear \"elbow\" at **K=5** (and potentially K=6). We will proceed with K=5 as it offers the best balance between cluster compactness and interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Model Training\n",
        "We initialize K-Means with `k-means++` to select intelligent starting centroids, which speeds up convergence and avoids poor clustering results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Model with K=5\n",
        "kmeans_final = KMeans(n_clusters=5, init='k-means++', random_state=42)\n",
        "df['Cluster'] = kmeans_final.fit_predict(X_scaled)\n",
        "\n",
        "print(\"Clustering completed. Labels assigned.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. 3D Visualization & Interpretation\n",
        "Since we used three features, a 2D plot would result in information loss. A 3D scatter plot allows us to view the separation of clusters across Age, Income, and Spending Score simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive 3D Plot\n",
        "fig = px.scatter_3d(df, \n",
        "                    x='Age', \n",
        "                    y='Annual Income (k$)', \n",
        "                    z='Spending Score (1-100)',\n",
        "                    color='Cluster', \n",
        "                    opacity=0.8, \n",
        "                    title=\"3D Cluster Visualization\",\n",
        "                    labels={'Cluster': 'Customer Segment'},\n",
        "                    color_continuous_scale=px.colors.qualitative.Bold)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Conclusion & Strategic Recommendations\n",
        "By analyzing the centroids of the derived clusters, we can profile the customer segments as follows:\n",
        "\n",
        "| Cluster | Characteristics (Age / Income / Spend) | Persona | Strategy |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **0** | Middle Age / Medium / Medium | **The Average Joe** | Standard promotions and retention strategies. |\n",
        "| **1** | Young / High / High | **The Elite** | VIP treatment, luxury brand marketing, and exclusive access. |\n",
        "| **2** | Old / High / Low | **The Conservative Wealthy** | Focus on \"Value for Money\" and quality assurance. |\n",
        "| **3** | Young / Low / High | **The Impulse Buyer** | Target with flash sales and trend-focused marketing. |\n",
        "| **4** | Any / Low / Low | **The Budget Conscious** | Offer discounts, coupons, and clearance sales. |\n",
        "\n",
        "**Future Work:** \n",
        "To improve this model, we could incorporate:\n",
        "* **Hierarchical Clustering** (Dendrograms) to validate K.\n",
        "* **Categorical Data:** Including Gender in the analysis using K-Modes or Gower Distance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
